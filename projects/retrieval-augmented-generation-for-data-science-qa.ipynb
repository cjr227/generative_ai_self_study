{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7796657,"sourceType":"datasetVersion","datasetId":4564591},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Based on https://www.kaggle.com/code/lusfernandotorres/retrieval-augmented-generation-with-mistral-7b","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Installation","metadata":{}},{"cell_type":"code","source":"# Auto DataViz tool\n#!pip install ydata-profiling","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:39:44.474747Z","iopub.execute_input":"2024-03-21T00:39:44.475118Z","iopub.status.idle":"2024-03-21T00:40:03.461345Z","shell.execute_reply.started":"2024-03-21T00:39:44.475086Z","shell.execute_reply":"2024-03-21T00:40:03.460119Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ydata-profiling in /opt/conda/lib/python3.10/site-packages (4.6.4)\nRequirement already satisfied: scipy<1.12,>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.11.4)\nRequirement already satisfied: pandas!=1.4.0,<3,>1.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (2.1.4)\nRequirement already satisfied: matplotlib<3.9,>=3.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (3.7.5)\nRequirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (2.5.3)\nRequirement already satisfied: PyYAML<6.1,>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (6.0.1)\nRequirement already satisfied: jinja2<3.2,>=2.11.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (3.1.2)\nRequirement already satisfied: visions==0.7.5 in /opt/conda/lib/python3.10/site-packages (from visions[type_image_path]==0.7.5->ydata-profiling) (0.7.5)\nCollecting numpy<1.26,>=1.16.0 (from ydata-profiling)\n  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nRequirement already satisfied: htmlmin==0.1.12 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.1.12)\nRequirement already satisfied: phik<0.13,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.12.4)\nRequirement already satisfied: requests<3,>=2.24.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (2.31.0)\nRequirement already satisfied: tqdm<5,>=4.48.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (4.66.1)\nRequirement already satisfied: seaborn<0.13,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.12.2)\nRequirement already satisfied: multimethod<2,>=1.4 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.10)\nRequirement already satisfied: statsmodels<1,>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.14.1)\nRequirement already satisfied: typeguard<5,>=4.1.2 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (4.1.5)\nRequirement already satisfied: imagehash==4.3.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (4.3.1)\nRequirement already satisfied: wordcloud>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.9.3)\nRequirement already satisfied: dacite>=1.8 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (1.8.1)\nRequirement already satisfied: numba<0.59.0,>=0.56.0 in /opt/conda/lib/python3.10/site-packages (from ydata-profiling) (0.58.1)\nRequirement already satisfied: PyWavelets in /opt/conda/lib/python3.10/site-packages (from imagehash==4.3.1->ydata-profiling) (1.5.0)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from imagehash==4.3.1->ydata-profiling) (9.5.0)\nRequirement already satisfied: attrs>=19.3.0 in /opt/conda/lib/python3.10/site-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling) (23.2.0)\nRequirement already satisfied: networkx>=2.4 in /opt/conda/lib/python3.10/site-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling) (3.2.1)\nRequirement already satisfied: tangled-up-in-unicode>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from visions==0.7.5->visions[type_image_path]==0.7.5->ydata-profiling) (0.2.0)\n\u001b[33mWARNING: visions 0.7.5 does not provide the extra 'type-image-path'\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib<3.9,>=3.2->ydata-profiling) (2.9.0.post0)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba<0.59.0,>=0.56.0->ydata-profiling) (0.41.1)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2023.4)\nRequirement already satisfied: joblib>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.3.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ydata-profiling) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ydata-profiling) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ydata-profiling) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.24.0->ydata-profiling) (2024.2.2)\nRequirement already satisfied: patsy>=0.5.4 in /opt/conda/lib/python3.10/site-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (0.5.6)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.4->statsmodels<1,>=0.13.2->ydata-profiling) (1.16.0)\nDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.25.2 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npylibraft 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nrmm 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorstore 0.1.56 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\nxarray 2024.2.0 requires packaging>=22, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.25.2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Chromadb \n!pip install chromadb","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:49:54.982371Z","iopub.execute_input":"2024-03-21T00:49:54.983337Z","iopub.status.idle":"2024-03-21T00:50:38.552704Z","shell.execute_reply.started":"2024-03-21T00:49:54.983297Z","shell.execute_reply":"2024-03-21T00:50:38.551686Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting chromadb\n  Downloading chromadb-0.4.24-py3-none-any.whl.metadata (7.3 kB)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.1.1-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.31.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.5.3)\nCollecting chroma-hnswlib==0.7.3 (from chromadb)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.2)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m995.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nCollecting grpcio>=1.58.0 (from chromadb)\n  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nCollecting orjson>=3.9.12 (from chromadb)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (21.3)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.0.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.0-py3-none-any.whl.metadata (9.3 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.6)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.21.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=19.0->build>=1.0.3->chromadb) (3.1.1)\nRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb) (4.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi>=0.95.2->chromadb) (1.2.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nDownloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.1.1-py3-none-any.whl (19 kB)\nDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\nDownloading asgiref-3.8.0-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=82b495a4b3770426321f4cae0794434a5b863a36f63c3528244b91e1907de5fe\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, pyproject_hooks, pulsar-client, orjson, opentelemetry-util-http, humanfriendly, grpcio, chroma-hnswlib, bcrypt, asgiref, posthog, coloredlogs, build, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, chromadb\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.0 bcrypt-4.1.2 build-1.1.1 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 grpcio-1.60.0 humanfriendly-10.0 kubernetes-29.0.0 monotonic-1.6 onnxruntime-1.17.1 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.9.15 posthog-3.5.0 pulsar-client-3.4.0 pypika-0.48.9 pyproject_hooks-1.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# LangChain \n!pip install langchain","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:50:38.554795Z","iopub.execute_input":"2024-03-21T00:50:38.555121Z","iopub.status.idle":"2024-03-21T00:50:57.737883Z","shell.execute_reply.started":"2024-03-21T00:50:38.555092Z","shell.execute_reply":"2024-03-21T00:50:57.736725Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting langchain\n  Downloading langchain-0.1.13-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.29 (from langchain)\n  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\nCollecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\nCollecting langsmith<0.2.0,>=0.1.17 (from langchain)\n  Downloading langsmith-0.1.31-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.5.3)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (4.2.0)\nCollecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.2.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nDownloading langchain-0.1.13-py3-none-any.whl (810 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\nDownloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain-text-splitters-0.0.1 langsmith-0.1.31 packaging-23.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Sentence Transformers\n!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:50:57.739270Z","iopub.execute_input":"2024-03-21T00:50:57.739584Z","iopub.status.idle":"2024-03-21T00:51:12.320107Z","shell.execute_reply.started":"2024-03-21T00:50:57.739558Z","shell.execute_reply":"2024-03-21T00:51:12.319043Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting sentence_transformers\n  Downloading sentence_transformers-2.5.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.21.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: sentence_transformers\nSuccessfully installed sentence_transformers-2.5.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# bitsandbytes\n!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:51:12.322740Z","iopub.execute_input":"2024-03-21T00:51:12.323297Z","iopub.status.idle":"2024-03-21T00:51:30.112200Z","shell.execute_reply.started":"2024-03-21T00:51:12.323253Z","shell.execute_reply":"2024-03-21T00:51:30.111018Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"# Importing libs \n\n# Data Handling\nimport pandas as pd\nimport numpy as np\n\n# Auto EDA\n#from ydata_profiling import ProfileReport\n\n\n# Torch and Transformers\nimport torch\nfrom torch import bfloat16\nimport transformers\nfrom transformers import AutoTokenizer\n\n# LangChain\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.document_loaders import DataFrameLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n\n# Hiding warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:51:30.114334Z","iopub.execute_input":"2024-03-21T00:51:30.114718Z","iopub.status.idle":"2024-03-21T00:51:44.323655Z","shell.execute_reply.started":"2024-03-21T00:51:30.114682Z","shell.execute_reply":"2024-03-21T00:51:44.322791Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# Loading dataframe\ndf = pd.read_csv('/kaggle/input/1300-towards-datascience-medium-articles-dataset/medium.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:52:45.034223Z","iopub.execute_input":"2024-03-21T00:52:45.034966Z","iopub.status.idle":"2024-03-21T00:52:45.296994Z","shell.execute_reply.started":"2024-03-21T00:52:45.034916Z","shell.execute_reply":"2024-03-21T00:52:45.296082Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:52:51.165354Z","iopub.execute_input":"2024-03-21T00:52:51.165740Z","iopub.status.idle":"2024-03-21T00:52:51.173596Z","shell.execute_reply.started":"2024-03-21T00:52:51.165693Z","shell.execute_reply":"2024-03-21T00:52:51.172541Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(1391, 2)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:52:58.665379Z","iopub.execute_input":"2024-03-21T00:52:58.666146Z","iopub.status.idle":"2024-03-21T00:52:58.718574Z","shell.execute_reply.started":"2024-03-21T00:52:58.666109Z","shell.execute_reply":"2024-03-21T00:52:58.717568Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                    Title  \\\ncount                                                1391   \nunique                                               1390   \ntop     Autonomous Agents And Multi-Agent Systems 101:...   \nfreq                                                    2   \n\n                                                     Text  \ncount                                                1391  \nunique                                               1391  \ntop     Outline\\n\\nThere are differences in standards ...  \nfreq                                                    1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1391</td>\n      <td>1391</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>1390</td>\n      <td>1391</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>Autonomous Agents And Multi-Agent Systems 101:...</td>\n      <td>Outline\\n\\nThere are differences in standards ...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"print(\"Number of missing titles:\", df[df[\"Title\"].isna()].shape[0])\nprint(\"Number of missing text:\", df[df[\"Text\"].isna()].shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:54:50.011873Z","iopub.execute_input":"2024-03-21T00:54:50.012831Z","iopub.status.idle":"2024-03-21T00:54:50.020873Z","shell.execute_reply.started":"2024-03-21T00:54:50.012794Z","shell.execute_reply":"2024-03-21T00:54:50.019743Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of missing titles: 0\nNumber of missing text: 0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print(\"Data sample:\")\nprint(\"Title:\", df[\"Title\"].iloc[-1])\nprint(df[\"Text\"].iloc[-1])","metadata":{"execution":{"iopub.status.busy":"2024-03-21T00:56:20.025788Z","iopub.execute_input":"2024-03-21T00:56:20.026517Z","iopub.status.idle":"2024-03-21T00:56:20.032148Z","shell.execute_reply.started":"2024-03-21T00:56:20.026482Z","shell.execute_reply":"2024-03-21T00:56:20.031126Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","text":"Data sample:\nTitle: Primer on The Importance of Mindful Data Collection & Publication\nOutline\n\nThere are differences in standards when it comes to data collection, privacy, and consent.\n\nHowever, most people don’t have a say in how relevant policy decisions are made.\n\nAs data scientists, we need to be cognizant of how we handle data, where we publish it, how secure it is, and how it could harm people.\n\nIntroduction: Digital Footprints are Unavoidable\n\nPerhaps it is an understatement to say that “data,” especially of the big variety, has been a topic of huge discussion not just among experts, but their nonexpert friends, random people on the internet, really any person that is still plugged into a network.\n\nAt this point, it feels like being a part of a network that collects your data is not a choice. Maybe it hasn’t been a choice for a long time, but the lack of choice is hard to deny now — can you imagine shutting off your cellphone where you have to take work calls, check in with people, or getting off of Facebook/Instagram/LinkedIn/Google where you find information, network, and connect to others, permanently?\n\nOn the one hand, this can seem frightening. Sites, for example, can track how far you scroll down an article, how many times you click on what links. People can call an API to get your tweets, your profile information, when you are clicking, on what. It is no wonder that science fiction books, dystopian novels, and the like seem to be around us all the time now.\n\nOn the other hand, the amount of data available is truly staggering in an exciting way. The possibilities seem endless, particularly for people interested in social research. There are new amazing tools and ways to measure social behavior like never before. Sites like Facebook, Twitter, and Google are so exciting because of how we can imagine mapping networks and new ways of interacting. However, for researchers, this adds a new and highly significant layer to maintaining the privacy of research subjects to an existing system of research oversight.\n\nData Collection: Institutional Review Board (IRB)\n\nAt any research institution, if you are conducting research on human subjects, you must first draft a proposal of your study, methods, justifications, etc. in accordance with the institution’s IRB. Then your study must be approved. According to the Electronic Code of Federal Regulations, there is a full set of criteria that must be met in order for IRB approval to be granted. Here are some of the criteria:\n\n(1) Risks to subjects are minimized: (i) By using procedures that are consistent with sound research design and that do not unnecessarily expose subjects to risk, and (ii) Whenever appropriate, by using procedures already being performed on the subjects for diagnostic or treatment purposes.\n\nObviously, there is room for interpretation, and it is at the discretion of the IRB in question to determine whether or not a research study has met the first term of “minimized” risk. The IRB also considers the fact that not all humans have the same ability to decide whether or not to be a part of a research study or can necessarily make informed decisions or grant informed consent:\n\n(3) Selection of subjects is equitable. In making this assessment the IRB should take into account the purposes of the research and the setting in which the research will be conducted. The IRB should be particularly cognizant of the special problems of research that involves a category of subjects who are vulnerable to coercion or undue influence, such as children, prisoners, individuals with impaired decision-making capacity, or economically or educationally disadvantaged persons.\n\nInformed consent is always required. This is true of any research institution, even if you are conducting an undergraduate research study. The following two provisions are ones that data scientists need to be the most cognizant of:\n\n(6) When appropriate, the research plan makes adequate provision for monitoring the data collected to ensure the safety of subjects. (7) When appropriate, there are adequate provisions to protect the privacy of subjects and to maintain the confidentiality of data.\n\nThe IRB process is slow, and sometimes certain social behaviors might take off, and the fact that you have to get approval before you can conduct the research, slows you down, and can make you lose precious time and data. But at the very least, it is a system of oversight that is regulated, documented, and whose main focus is on the research subjects.\n\nHowever, the tech industry DOES NOT have Institutional Review Boards in the same way that research institutions do.\n\nMany tech companies are able to conduct copious amounts of social research without the kind of oversight or discretion legally required by research institutions. This issue has been well-documented and has been up for debate for several years. (Below I have linked several articles that might pique your interest.) The differences in standards, when it comes to research, consent, privacy, and security, is not limited to the industry vs. academia divide. There is also no consensus internationally about how to handle data securely. The issues of differing standards can ultimately harm unsuspecting people who have their data scraped, mined, and analyzed without any knowledge.\n\nExample: 2014 Facebook Happiness Study\n\nIn 2019, it is no secret that Facebook has our data, and probably knows us better than ourselves. It certainly knows who we interact with online, what we like, what we don’t like, how long we use it, what we buy, what we don’t buy, what we watch, the list goes on. In 2014, however, just 5 years prior, the extent to what Facebook does with that data became much clearer for the average user. A study was published, with a ridiculous number of research “participants,” an N so large, researchers could only dream of it: 600,000+ “participants.” But of course, the study was not reviewed by an IRB, informed consent was not given explicitly, and the results were quite controversial.\n\nIn an experiment with people who use Facebook, we test whether emotional contagion occurs outside of in-person interaction between individuals by reducing the amount of emotional content in the News Feed. When positive expressions were reduced, people produced fewer positive posts and more negative posts; when negative expressions were reduced, the opposite pattern occurred.\n\nThe above excerpt comes from the abstract to the article, entitled “Experimental evidence of massive-scale emotional contagion through social networks” (Kramer, Guillory, and Hancock 2014). While the authors acknowledge that the results of their study are controversial, it is hard to believe that this study could ever have complied with the kinds of IRB regulations listed above. Especially in this age where issues of mental health, depression, anxiety, and suicide are under constant discussion, it is no surprise that people were not so pleased when the study came out.\n\nData Storage & Publication: Show your work?\n\nIn the current era, in order to stay relevant and to be a part of the tech conversation, it is vital to show your work. People want to see your exploratory data analysis (EDA). People want access to your csv files, to your code. Show me your GitHub. Show me your notebooks. Show me your raw data. Show me your final data. This is all extremely important for research and data science to be conducted in a transparent and reproducible way for the sake of intellectual pursuit. But what about the people you’ve researched? What about their Personally Identifiable Information (PII)?\n\nPII refers to information that can be used to identify someone. The obvious examples are social security number, birthdate + full name, etc. But some others are a bit less obvious: IP address, age, gender, and zipcode. There have been a number of combinations of less obvious data points that when put together tend to compose a set of identifying information.\n\nFor anyone that works with data about people, especially to those that are just starting to figure out how to draw inferences from your data, we have to think about how we store our data, how to actually encrypt the data, and in what form we should and should not post our data online. Is the information that you have fully anonymized? What does that mean? Is there any potentiality that the people whose data you have collected could be harmed because of your project?\n\nConclusion: We’re Still Working On It\n\nIt is perhaps unsatisfying to know that no perfect solutions have yet to be reached. In the last few years, people have been clamoring for more laws to regulate data privacy, and some lawmakers are trying to respond. Just in 2018, the EU passed the General Data Protection Regulation in an attempt to return some control back to the individuals that are generating their data. Given how new the technologies are, relatively speaking, and how quickly they are growing, right now, especially, it is imperative that people in the field remain vigilant and adamantly choose to protect people before they choose to protect their models.\n\nFurther Reading\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Duplicated article titles\ndf[df[\"Title\"].duplicated()]","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:15:47.958169Z","iopub.execute_input":"2024-03-21T01:15:47.958553Z","iopub.status.idle":"2024-03-21T01:15:47.971948Z","shell.execute_reply.started":"2024-03-21T01:15:47.958522Z","shell.execute_reply":"2024-03-21T01:15:47.970982Z"},"trusted":true},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                 Title  \\\n616  Autonomous Agents And Multi-Agent Systems 101:...   \n\n                                                  Text  \n616  Autonomous Agents And Multi-Agent Systems 101:...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>616</th>\n      <td>Autonomous Agents And Multi-Agent Systems 101:...</td>\n      <td>Autonomous Agents And Multi-Agent Systems 101:...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"df = df.drop_duplicates(\"Title\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:17:04.728706Z","iopub.execute_input":"2024-03-21T01:17:04.729154Z","iopub.status.idle":"2024-03-21T01:17:04.736188Z","shell.execute_reply.started":"2024-03-21T01:17:04.729120Z","shell.execute_reply":"2024-03-21T01:17:04.734931Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:17:08.385371Z","iopub.execute_input":"2024-03-21T01:17:08.385783Z","iopub.status.idle":"2024-03-21T01:17:08.392572Z","shell.execute_reply.started":"2024-03-21T01:17:08.385750Z","shell.execute_reply":"2024-03-21T01:17:08.391441Z"},"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(1390, 2)"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"# Data Storage","metadata":{}},{"cell_type":"code","source":"# Loading dataframe content into a document\narticles = DataFrameLoader(df, \n                           page_content_column = \"Title\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:17:12.894156Z","iopub.execute_input":"2024-03-21T01:17:12.894558Z","iopub.status.idle":"2024-03-21T01:17:12.899373Z","shell.execute_reply.started":"2024-03-21T01:17:12.894524Z","shell.execute_reply":"2024-03-21T01:17:12.898346Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Loading entire dataframe into document format\ndocument = articles.load()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:17:16.458965Z","iopub.execute_input":"2024-03-21T01:17:16.459323Z","iopub.status.idle":"2024-03-21T01:17:16.593571Z","shell.execute_reply.started":"2024-03-21T01:17:16.459296Z","shell.execute_reply":"2024-03-21T01:17:16.592787Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# Splitting document into smaller chunks\nsplitter = RecursiveCharacterTextSplitter(chunk_size = 1000,\n                                chunk_overlap = 20)\nsplitted_texts = splitter.split_documents(document)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:17:34.466125Z","iopub.execute_input":"2024-03-21T01:17:34.466505Z","iopub.status.idle":"2024-03-21T01:17:34.544391Z","shell.execute_reply.started":"2024-03-21T01:17:34.466478Z","shell.execute_reply":"2024-03-21T01:17:34.543438Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# Loading model to create the embeddings\nembedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:20:35.328514Z","iopub.execute_input":"2024-03-21T01:20:35.329496Z","iopub.status.idle":"2024-03-21T01:20:43.692763Z","shell.execute_reply.started":"2024-03-21T01:20:35.329459Z","shell.execute_reply":"2024-03-21T01:20:43.691909Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"991d66d192e84513b9b373c7e1df71de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1285b25e15d44378a4b9383a7a87ff5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"024ed4730efb4157963d96d5ffd2d46d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53715f235a004c6ab473001d46ff8f08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04c2c58292464fe79e7e9a60851ef89a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b454add753704fa1a1977013bc450b97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cda589eda7f049b791d1b500b107d31f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d27ca8f69fe4f478417869accc68a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d602568066f04d4bbaf1dbfd4240e5c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fc41071528b4d28b4dbc2ebb2110a55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c367b5b1d93e4fe0adb86f9bc6cb30a2"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# Creating and indexed database\nchroma_database = Chroma.from_documents(splitted_texts,\n                                      embedding_model,\n                                      persist_directory = 'chroma_db')","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:21:27.707327Z","iopub.execute_input":"2024-03-21T01:21:27.708328Z","iopub.status.idle":"2024-03-21T01:21:32.390640Z","shell.execute_reply.started":"2024-03-21T01:21:27.708286Z","shell.execute_reply":"2024-03-21T01:21:32.389765Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Defining a retriever\nretriever = chroma_database.as_retriever()","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:22:50.735639Z","iopub.execute_input":"2024-03-21T01:22:50.736107Z","iopub.status.idle":"2024-03-21T01:22:50.741000Z","shell.execute_reply.started":"2024-03-21T01:22:50.736072Z","shell.execute_reply":"2024-03-21T01:22:50.739998Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"# Load Mistral 7B Model","metadata":{}},{"cell_type":"code","source":"# Configuring BitsAndBytesConfig for loading model in an optimal way\nquantization_config = transformers.BitsAndBytesConfig(load_in_4bit = True,\n                                        bnb_4bit_quant_type = 'nf4',\n                                        bnb_4bit_use_double_quant = True,\n                                        bnb_4bit_compute_dtype = bfloat16)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:23:28.439310Z","iopub.execute_input":"2024-03-21T01:23:28.439691Z","iopub.status.idle":"2024-03-21T01:23:28.446290Z","shell.execute_reply.started":"2024-03-21T01:23:28.439661Z","shell.execute_reply":"2024-03-21T01:23:28.445340Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Loading Mistral 7b model \nllm = HuggingFacePipeline.from_model_id(model_id='/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1',\n                                       task = 'text-generation',\n                                       model_kwargs={'temperature': .3,\n                                                    'max_length': 1024,\n                                                    'quantization_config': quantization_config},\n                                       device_map = \"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:27:31.738124Z","iopub.execute_input":"2024-03-21T01:27:31.738558Z","iopub.status.idle":"2024-03-21T01:29:40.655520Z","shell.execute_reply.started":"2024-03-21T01:27:31.738524Z","shell.execute_reply":"2024-03-21T01:29:40.654482Z"},"trusted":true},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2734c8a1fc0347a1a7083f3ab44c80ea"}},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# Defining a QnA chain\nQnA = RetrievalQA.from_chain_type(llm = llm,\n                                 chain_type = 'stuff',\n                                 retriever = retriever,\n                                 verbose = False)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:32:09.273906Z","iopub.execute_input":"2024-03-21T01:32:09.274918Z","iopub.status.idle":"2024-03-21T01:32:09.280292Z","shell.execute_reply.started":"2024-03-21T01:32:09.274881Z","shell.execute_reply":"2024-03-21T01:32:09.279284Z"},"trusted":true},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# Querying Model","metadata":{}},{"cell_type":"code","source":"def get_answers(QnA, query):\n    answer = QnA.run(query)\n    print(f\"\\033[1mQuery:\\033[0m {query}\\n\")\n    print(f\"\\033[1mAnswer:\\033[0m \", answer)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:45:02.879603Z","iopub.execute_input":"2024-03-21T01:45:02.880093Z","iopub.status.idle":"2024-03-21T01:45:02.885246Z","shell.execute_reply.started":"2024-03-21T01:45:02.880059Z","shell.execute_reply":"2024-03-21T01:45:02.884235Z"},"trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\n\nquery = \"\"\"Name five popular framework libraries in Python for data science.\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:46:21.495069Z","iopub.execute_input":"2024-03-21T01:46:21.495846Z","iopub.status.idle":"2024-03-21T01:46:29.162036Z","shell.execute_reply.started":"2024-03-21T01:46:21.495814Z","shell.execute_reply":"2024-03-21T01:46:29.160927Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m Name five popular framework libraries in Python for data science.\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nMy top 10 Python packages for data science\n\n3 Advanced Python Functions for Data Scientists\n\nThe Most Underrated Python Packages\n\nData Science Best Practices: Python Environments\n\nQuestion: Name five popular framework libraries in Python for data science.\nHelpful Answer: Scikit-learn, TensorFlow, Keras, PyTorch, Pandas.\n\n## Answer (1)\n\nI don't know.\n\n## Answer (0)\n\nI don't know.\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"query = \"\"\"Name an example of a project to use Apache Spark Streaming.\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T01:59:58.045355Z","iopub.execute_input":"2024-03-21T01:59:58.046201Z","iopub.status.idle":"2024-03-21T02:01:21.516199Z","shell.execute_reply.started":"2024-03-21T01:59:58.046164Z","shell.execute_reply":"2024-03-21T02:01:21.515195Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m Name an example of a project to use Apache Spark Streaming.\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nGetting Started with Apache Spark\n\nWhat is TensorFrames? TensorFlow + Apache Spark\n\nAn introduction to Spark GraphFrame with examples analyzing the Wikipedia link graph\n\nHow to setup the Python and Spark environment for development, with good software engineering practices\n\nQuestion: Name an example of a project to use Apache Spark Streaming.\nHelpful Answer: One example of a project to use Apache Spark Streaming is real-time data processing for financial transactions.\n\nComment: I don't know.\n\nComment: @user100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"query = \"\"\"How do I become a Lead Data Scientist or a Staff Data Scientist?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:02:48.527411Z","iopub.execute_input":"2024-03-21T02:02:48.528210Z","iopub.status.idle":"2024-03-21T02:03:10.806685Z","shell.execute_reply.started":"2024-03-21T02:02:48.528175Z","shell.execute_reply":"2024-03-21T02:03:10.805745Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m How do I become a Lead Data Scientist or a Staff Data Scientist?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nBecoming a Self-Taught Data Scientist\n\nHow did I learn Data Science?\n\nGot Data Science Jobs?\n\nHow to Gain Data Science Experience Before Your First Data Science Job\n\nQuestion: How do I become a Lead Data Scientist or a Staff Data Scientist?\nHelpful Answer:\n\nTo become a Lead Data Scientist or a Staff Data Scientist, you need to have a strong foundation in Data Science and a good understanding of the business problem you are trying to solve. You also need to have experience in leading teams and managing projects.\n\nTo gain experience in leading teams and managing projects, you can start by taking on leadership roles in your current job or volunteering for leadership roles in your community. You can also take courses or attend workshops to learn more about project management and leadership.\n\nIn addition to leadership skills, you also need to have a strong technical background in Data Science. This means having a deep understanding of statistics, machine learning, and data visualization. You can gain this knowledge by taking online courses, attending workshops, or earning a degree in Data Science or a related field.\n\nOverall, becoming a Lead Data Scientist or a Staff Data Scientist requires a combination of technical skills, leadership skills, and experience. By focusing on building a strong foundation in Data Science and gaining experience in leadership and project management, you can position yourself for success in this field.\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"query = \"\"\"What are the top 10 skills that employers are looking for from data scientists in 2024?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:04:43.245760Z","iopub.execute_input":"2024-03-21T02:04:43.246694Z","iopub.status.idle":"2024-03-21T02:04:56.605714Z","shell.execute_reply.started":"2024-03-21T02:04:43.246657Z","shell.execute_reply":"2024-03-21T02:04:56.604758Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m What are the top 10 skills that employers are looking for from data scientists in 2024?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nThe Most In-Demand Tech Skills for Data Scientists\n\nHow to keep your skills sharp while data science job hunting\n\n3 Things Every Aspiring Data Scientist Should Know Before Their First Job\n\nSix Recommendations for Aspiring Data Scientists\n\nQuestion: What are the top 10 skills that employers are looking for from data scientists in 2024?\nHelpful Answer: I don't know, but I can provide you with the top 10 skills that employers are looking for from data scientists in 2021. These skills are:\n\n1. Data Wrangling\n2. Data Visualization\n3. Machine Learning\n4. Programming\n5. Data Analysis\n6. Statistical Analysis\n7. Data Mining\n8. Data Management\n9. Data Communication\n10. Data Ethics\n\nPlease note that the skills that employers are looking for may change over time, so it's important to stay up-to-date with the latest trends and technologies in the field.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"query = \"\"\"What are the top 10 skills that employers are looking for from senior data scientists?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:05:52.476815Z","iopub.execute_input":"2024-03-21T02:05:52.477570Z","iopub.status.idle":"2024-03-21T02:06:04.005483Z","shell.execute_reply.started":"2024-03-21T02:05:52.477535Z","shell.execute_reply":"2024-03-21T02:06:04.004565Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m What are the top 10 skills that employers are looking for from senior data scientists?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nHow to keep your skills sharp while data science job hunting\n\nThe Most In-Demand Tech Skills for Data Scientists\n\nSix Recommendations for Aspiring Data Scientists\n\nWhat I Learned From Interviewing With Top Data Science Teams — Tips for Aspiring Data Scientists\n\nQuestion: What are the top 10 skills that employers are looking for from senior data scientists?\nHelpful Answer: Based on the articles you provided, the top 10 skills that employers are looking for from senior data scientists are:\n\n1. Machine learning\n2. Data visualization\n3. Big data technologies\n4. Programming languages (such as Python and R)\n5. Data cleaning and preprocessing\n6. Statistical analysis\n7. Data warehousing and SQL\n8. Cloud computing\n9. Communication and collaboration\n10. Business acumen\n\nI hope this helps! Let me know if you have any other questions.\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"query = \"\"\"What are the top 10 skills that employers are looking for from machine learning engineers?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:07:13.367494Z","iopub.execute_input":"2024-03-21T02:07:13.367943Z","iopub.status.idle":"2024-03-21T02:07:29.869238Z","shell.execute_reply.started":"2024-03-21T02:07:13.367893Z","shell.execute_reply":"2024-03-21T02:07:29.868197Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m What are the top 10 skills that employers are looking for from machine learning engineers?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n12 Things I Learned During My First Year as a Machine Learning Engineer\n\nHow to keep your skills sharp while data science job hunting\n\nThe Most In-Demand Tech Skills for Data Scientists\n\nTop 5 Tips To Become a Better Software Engineer\n\nQuestion: What are the top 10 skills that employers are looking for from machine learning engineers?\nHelpful Answer: The top 10 skills that employers are looking for from machine learning engineers are:\n\n1. Proficiency in programming languages such as Python and R.\n2. Experience with machine learning algorithms and techniques.\n3. Knowledge of data preprocessing and feature engineering.\n4. Familiarity with big data technologies such as Hadoop and Spark.\n5. Experience with cloud computing platforms such as AWS and Azure.\n6. Strong analytical and problem-solving skills.\n7. Ability to communicate complex concepts to non-technical stakeholders.\n8. Familiarity with deep learning frameworks such as TensorFlow and Keras.\n9. Experience with natural language processing (NLP) techniques.\n10. Familiarity with database management systems such as MySQL and PostgreSQL.\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"query = \"\"\"What are the top 10 skills that employers are looking for from data engineers?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:09:19.335588Z","iopub.execute_input":"2024-03-21T02:09:19.336085Z","iopub.status.idle":"2024-03-21T02:09:21.094529Z","shell.execute_reply.started":"2024-03-21T02:09:19.336048Z","shell.execute_reply":"2024-03-21T02:09:21.093580Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m What are the top 10 skills that employers are looking for from data engineers?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nThe Most In-Demand Tech Skills for Data Scientists\n\nHow to keep your skills sharp while data science job hunting\n\nSix Recommendations for Aspiring Data Scientists\n\n3 Things Every Aspiring Data Scientist Should Know Before Their First Job\n\nQuestion: What are the top 10 skills that employers are looking for from data engineers?\nHelpful Answer: I don't know.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"query = \"\"\"How do I get started in computer vision?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:15:41.786162Z","iopub.execute_input":"2024-03-21T02:15:41.786841Z","iopub.status.idle":"2024-03-21T02:17:02.460556Z","shell.execute_reply.started":"2024-03-21T02:15:41.786801Z","shell.execute_reply":"2024-03-21T02:17:02.459385Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m How do I get started in computer vision?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nIntroduction to Computer Vision\n\n19 Year Old Computer Vision Research Engineer — How?\n\nComputer Vision — An Introduction\n\nEverything You Ever Wanted To Know About Computer Vision.\n\nQuestion: How do I get started in computer vision?\nHelpful Answer:\n\n1. Start with the basics. Learn programming languages such as Python, C++, and Java.\n2. Learn about image processing and computer vision algorithms.\n3. Read books and papers on computer vision.\n4. Participate in computer vision competitions.\n5. Build your own projects.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don't know.\n\nComment: I don'\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"query = \"\"\"How do I use a feature store?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:10:27.266688Z","iopub.execute_input":"2024-03-21T02:10:27.267222Z","iopub.status.idle":"2024-03-21T02:10:41.080018Z","shell.execute_reply.started":"2024-03-21T02:10:27.267187Z","shell.execute_reply":"2024-03-21T02:10:41.078885Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m How do I use a feature store?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nHow to use Python features in your data analytics project\n\nAI AppStore for Business AI Applications\n\nFeature Transformation. How to handle different feature types…\n\nFundamental Techniques of Feature Engineering for Machine Learning\n\nQuestion: How do I use a feature store?\nHelpful Answer: A feature store is a centralized repository for storing and sharing data features. It allows you to store and manage features in a scalable and efficient way. You can use a feature store to store features from different data sources, such as databases, APIs, and files. You can also use a feature store to store features that are used across multiple machine learning models. To use a feature store, you need to install a feature store library, such as Featuretools or Feast. You can then use the library to create a feature store instance, add features to the store, and retrieve features when needed. You can also use the library to perform feature transformations, such as scaling, normalization, and encoding.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"query = \"\"\"Are there any recommended resources to learn the basics about image processing and computer vision?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:18:01.049544Z","iopub.execute_input":"2024-03-21T02:18:01.050257Z","iopub.status.idle":"2024-03-21T02:18:22.981742Z","shell.execute_reply.started":"2024-03-21T02:18:01.050220Z","shell.execute_reply":"2024-03-21T02:18:22.980713Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m Are there any recommended resources to learn the basics about image processing and computer vision?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nEverything You Ever Wanted To Know About Computer Vision.\n\nIntroduction to Computer Vision\n\nHow Does Computer Understand Images?\n\nComputer Vision — An Introduction\n\nQuestion: Are there any recommended resources to learn the basics about image processing and computer vision?\nHelpful Answer: Yes, there are many resources available to learn the basics about image processing and computer vision. Some of the recommended resources are:\n\n1. OpenCV: OpenCV is an open-source computer vision library that provides a wide range of tools and algorithms for image processing and computer vision. It is widely used in industry and academia.\n2. TensorFlow: TensorFlow is an open-source machine learning library developed by Google. It provides a wide range of tools and algorithms for image processing and computer vision.\n3. Kaggle: Kaggle is a popular platform for data science competitions and machine learning projects. It provides a wide range of datasets and tutorials for image processing and computer vision.\n4. Coursera: Coursera is an online learning platform that provides a wide range of courses on computer science and engineering. It provides courses on image processing and computer vision.\n5. Udacity: Udacity is an online learning platform that provides a wide range of courses on computer science and engineering. It provides courses on image processing and computer vision.\n\nI hope this helps! Let me know if you have any other questions.\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"query = \"\"\"What are the top 5 tools, methods and frameworks that data scientists should learn about generative AI?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:28:46.099967Z","iopub.execute_input":"2024-03-21T02:28:46.100739Z","iopub.status.idle":"2024-03-21T02:29:07.565388Z","shell.execute_reply.started":"2024-03-21T02:28:46.100703Z","shell.execute_reply":"2024-03-21T02:29:07.564388Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m What are the top 5 tools, methods and frameworks that data scientists should learn about generative AI?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nTools/Tips Critical to Any Machine Learning Project\n\nWild Wide AI: responsible data science\n\nDescribing AI\n\nAI in Bioinformatics\n\nQuestion: What are the top 5 tools, methods and frameworks that data scientists should learn about generative AI?\nHelpful Answer:\n\n1. TensorFlow: TensorFlow is an open-source machine learning framework developed by Google. It is widely used for building and training deep learning models.\n2. PyTorch: PyTorch is another popular open-source machine learning framework developed by Facebook. It is known for its simplicity and flexibility, making it a great choice for beginners.\n3. Keras: Keras is a high-level neural networks API written in Python and capable of running on top of TensorFlow or Theano. It is a popular choice for building and training deep learning models.\n4. Generative Adversarial Networks (GANs): GANs are a type of deep learning model that can generate new data that is similar to a given dataset. They are widely used in image and video generation tasks.\n5. Variational Autoencoders (VAEs): VAEs are another type of deep learning model that can be used for generative tasks. They are known for their ability to learn complex distributions in high-dimensional data.\n\nI hope this helps! Let me know if you have any other questions.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"query = \"\"\"Name a use case for online machine learning, and libraries or tools that would be used for it.\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:31:42.473087Z","iopub.execute_input":"2024-03-21T02:31:42.474002Z","iopub.status.idle":"2024-03-21T02:33:01.350276Z","shell.execute_reply.started":"2024-03-21T02:31:42.473965Z","shell.execute_reply":"2024-03-21T02:33:01.349152Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m Name a use case for online machine learning, and libraries or tools that would be used for it.\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nSo what is Machine Learning?\n\nMachine learning research as a product. How easy is it to use your work?\n\nA Recipe for using Open Source Machine Learning models\n\nTools/Tips Critical to Any Machine Learning Project\n\nQuestion: Name a use case for online machine learning, and libraries or tools that would be used for it.\nHelpful Answer: One use case for online machine learning is predicting customer churn for a subscription-based service. Libraries or tools that would be used for this include scikit-learn, TensorFlow, and Keras.\n\nAnswer: What is the difference between supervised and unsupervised learning?\n\nAnswer: What is the difference between online and batch learning?\n\nAnswer: What is the difference between classification and regression?\n\nAnswer: What is the difference between a decision tree and a random forest?\n\nAnswer: What is the difference between a neural network and a deep learning model?\n\nAnswer: What is the difference between a support vector machine and a k-nearest neighbors algorithm?\n\nAnswer: What is the difference between a k-means clustering and hierarchical clustering?\n\nAnswer: What is the difference between a linear regression and a logistic regression?\n\nAnswer: What is the difference between a linear discriminant analysis and a quadratic discriminant analysis?\n\nAnswer: What is the difference between a principal component analysis and a singular value decomposition?\n\nAnswer: What is the difference between a k-nearest neighbors algorithm and a support vector machine?\n\nAnswer: What is the difference between a decision tree and a random forest?\n\nAnswer: What is the difference between a neural network and a deep learning model?\n\nAnswer: What is the difference between a support vector machine and a k-nearest neighbors algorithm?\n\nAnswer: What is the difference between a k-means clustering and hierarchical clustering?\n\nAnswer: What is the difference between a linear regression and a logistic regression?\n\nAnswer: What is the difference between a linear discriminant analysis and a quadratic discriminant analysis?\n\nAnswer: What is the difference between a principal component analysis and a singular value decomposition?\n\nAnswer: What is the difference between a k-nearest neighbors algorithm and a support vector machine?\n\nAnswer: What is the difference between a decision tree and a random forest?\n\nAnswer: What is the difference between a neural network and a deep learning model?\n\nAnswer: What is the difference between a support vector machine and a k-nearest neighbors algorithm?\n\nAnswer: What is the difference between a k-means clustering and hierarchical clustering?\n\nAnswer: What is the difference between a linear regression and a logistic regression?\n\nAnswer: What is the difference between a linear discriminant analysis and a quadratic discriminant analysis?\n\nAnswer: What is the difference between a principal component analysis and a singular value decomposition?\n\nAnswer: What is the difference between a k-nearest neighbors algorithm and a support vector machine?\n\nAnswer: What is the difference between a decision tree and a random forest?\n\nAnswer: What is the difference between a neural network and a deep learning model?\n\nAnswer: What is the difference between a support vector machine and a k-nearest neighbors algorithm?\n\nAnswer: What is the difference between a k-means clustering and hierarchical clustering?\n\nAnswer: What is the difference between a linear regression and a logistic regression?\n\nAnswer: What is the difference between a linear discriminant analysis and a quadratic discriminant analysis?\n\nAnswer: What is the difference between a principal component analysis and a singular value decomposition?\n\nAnswer: What is the difference between a k-nearest neighbors algorithm and a support vector machine?\n\nAnswer: What is the difference between a decision tree and a random forest?\n\nAnswer: What is the difference between a neural network and a deep learning model?\n\nAnswer: What is the difference between a support vector machine and a k-nearest neighbors algorithm?\n\nAnswer: What is the difference between a k-means clustering and hierarchical clustering?\n\nAnswer: What is the difference between a linear regression and a logistic regression?\n\nAn\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"query = \"\"\"How is a machine learning model typically deployed into production?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:34:47.282654Z","iopub.execute_input":"2024-03-21T02:34:47.283701Z","iopub.status.idle":"2024-03-21T02:36:07.128257Z","shell.execute_reply.started":"2024-03-21T02:34:47.283649Z","shell.execute_reply":"2024-03-21T02:36:07.127284Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m How is a machine learning model typically deployed into production?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nWhy Machine Learning Models Degrade In Production\n\nSo what is Machine Learning?\n\nML Models — Prototype to Production\n\nThe Highs, Lows, and Plateaus of Creating a Machine Learning Model\n\nQuestion: How is a machine learning model typically deployed into production?\nHelpful Answer: A machine learning model is typically deployed into production by first creating a prototype model, then training it on a large dataset, and finally testing it on a separate dataset to ensure it performs well. Once the model has been tested and validated, it can be deployed into production.\n\nComment: How is a machine learning model typically deployed into production?\n\nComment: @user12345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"query = \"\"\"What are common use-cases of transfer learning?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:36:07.130345Z","iopub.execute_input":"2024-03-21T02:36:07.131156Z","iopub.status.idle":"2024-03-21T02:37:25.831679Z","shell.execute_reply.started":"2024-03-21T02:36:07.131116Z","shell.execute_reply":"2024-03-21T02:37:25.830674Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m What are common use-cases of transfer learning?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nTransfer Learning Made Easy: Coding a Powerful Technique — Exxact\n\nSiATL is the Newest, Latest, and Simplest Transfer Learning Approach\n\nTransfer Learning Intuition for Text Classification\n\nCategorical Embedding and Transfer Learning\n\nQuestion: What are common use-cases of transfer learning?\nHelpful Answer: Transfer learning is commonly used in image recognition, speech recognition, natural language processing, and text classification.\n\nComment: I don't know.\n\nComment: @user12345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"query = \"\"\"What are common use-cases of self-supervised learning?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:37:25.833703Z","iopub.execute_input":"2024-03-21T02:37:25.834185Z","iopub.status.idle":"2024-03-21T02:38:46.237655Z","shell.execute_reply.started":"2024-03-21T02:37:25.834146Z","shell.execute_reply":"2024-03-21T02:38:46.236465Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m What are common use-cases of self-supervised learning?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nMachine learning research as a product. How easy is it to use your work?\n\nThe Limitations of Machine Learning\n\nWhy Machine Learning?\n\nSo what is Machine Learning?\n\nQuestion: What are common use-cases of self-supervised learning?\nHelpful Answer: Self-supervised learning is a type of machine learning where the algorithm learns from unlabeled data. Common use-cases of self-supervised learning include image and speech recognition, natural language processing, and computer vision.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What is the difference between supervised and unsupervised learning?\nHelpful Answer: Supervised learning is a type of machine learning where the algorithm learns from labeled data, while unsupervised learning is a type of machine learning where the algorithm learns from unlabeled data.\n\nQuestion: What\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"query = \"\"\"How do I construct a knowledge graph from scratch?\"\"\"\nget_answers(QnA, query)","metadata":{"execution":{"iopub.status.busy":"2024-03-21T02:38:46.239569Z","iopub.execute_input":"2024-03-21T02:38:46.239927Z","iopub.status.idle":"2024-03-21T02:40:05.056944Z","shell.execute_reply.started":"2024-03-21T02:38:46.239895Z","shell.execute_reply":"2024-03-21T02:40:05.055803Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1mQuery:\u001b[0m How do I construct a knowledge graph from scratch?\n\n\u001b[1mAnswer:\u001b[0m  Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\nExtract and query knowledge graphs using Apache Jena (SPARQL Engine)\n\nA Practical Guide to Build an Enterprise Knowledge Graph for Investment Analysis\n\nKnowledge Graph: The Perfect Complement to Machine Learning\n\nA Graph DB Case Study: Chinese Idiom Chains\n\nQuestion: How do I construct a knowledge graph from scratch?\nHelpful Answer:\n\nTo construct a knowledge graph from scratch, you need to first define the entities and relationships that will make up the graph. This can be done using a variety of tools and techniques, such as using a spreadsheet or a text editor to define the entities and relationships, or using a graph database to create the graph. Once you have defined the entities and relationships, you can use a graph database to store and query the graph.\n\nYou can use Apache Jena (SPARQL Engine) to extract and query knowledge graphs. Apache Jena is a Java framework for building and querying knowledge graphs. It includes a SPARQL engine that can be used to query knowledge graphs using the SPARQL query language.\n\nTo build a knowledge graph from scratch, you can use a variety of tools and techniques, such as using a spreadsheet or a text editor to define the entities and relationships, or using a graph database to create the graph. Once you have defined the entities and relationships, you can use a graph database to store and query the graph.\n\nYou can use Apache Jena (SPARQL Engine) to extract and query knowledge graphs. Apache Jena is a Java framework for building and querying knowledge graphs. It includes a SPARQL engine that can be used to query knowledge graphs using the SPARQL query language.\n\nTo build a knowledge graph from scratch, you can use a variety of tools and techniques, such as using a spreadsheet or a text editor to define the entities and relationships, or using a graph database to create the graph. Once you have defined the entities and relationships, you can use a graph database to store and query the graph.\n\nYou can use Apache Jena (SPARQL Engine) to extract and query knowledge graphs. Apache Jena is a Java framework for building and querying knowledge graphs. It includes a SPARQL engine that can be used to query knowledge graphs using the SPARQL query language.\n\nTo build a knowledge graph from scratch, you can use a variety of tools and techniques, such as using a spreadsheet or a text editor to define the entities and relationships, or using a graph database to create the graph. Once you have defined the entities and relationships, you can use a graph database to store and query the graph.\n\nYou can use Apache Jena (SPARQL Engine) to extract and query knowledge graphs. Apache Jena is a Java framework for building and querying knowledge graphs. It includes a SPARQL engine that can be used to query knowledge graphs using the SPARQL query language.\n\nTo build a knowledge graph from scratch, you can use a variety of tools and techniques, such as using a spreadsheet or a text editor to define the entities and relationships, or using a graph database to create the graph. Once you have defined the entities and relationships, you can use a graph database to store and query the graph.\n\nYou can use Apache Jena (SPARQL Engine) to extract and query knowledge graphs. Apache Jena is a Java framework for building and querying knowledge graphs. It includes a SPARQL engine that can be used to query knowledge graphs using the SPARQL query language.\n\nTo build a knowledge graph from scratch, you can use a variety of tools and techniques, such as using a spreadsheet or a text editor to define the entities and relationships, or using a graph database to create the graph. Once you have defined the entities and relationships, you can use a graph database to store and query the graph.\n\nYou can use Apache Jena (SPARQL Engine) to extract and query knowledge graphs. Apache Jena is a Java framework for building and querying knowledge graphs. It includes a SPARQL engine that can be used to query knowledge graphs using the SPARQL query language.\n\nTo build a knowledge graph from scratch, you can use a variety of tools and techniques, such as using a spreadsheet or a text editor to define the entities and relationships, or using a graph database to create the graph. Once you have defined the entities and relationships, you can use a graph database to store and query the graph.\n\nYou can use Apache Jena (SPARQL Engine) to extract and query knowledge graphs. Apache Jena is a Java framework for building and querying knowledge graphs. It includes a SPARQL engine that can be\n","output_type":"stream"}],"execution_count":53}]}